# 量化模型



## 安装环境
```
pip install -r requirements.txt
```



## 将模型量化为f32

```
python convert.py D:\models\Qwen1.5-0.5B-Chat --outtype f32 --vocab-type bpe --pad-vocab
```

## 将模型量化为f32

```
python convert-hf-to-gguf.py D:\models\Qwen1.5-0.5B-Chat  --outtype f32
```

## 将模型量化为f16

```
python convert-hf-to-gguf.py C:\ai\Yi-1.5-34B
```

## 将模型量化为Q6_K
```
.\build\bin\Release\quantize.exe D:\models\Qwen1.5-14B-Chat-f16.gguf D:\models\Qwen1.5-14B-Chat-Q6_K.gguf Q6_K
```

### 可选项

详情可以看 examples\quantize\README.md

| Quantization | Bits per Weight (BPW) |
|--------------|-----------------------|
| Q2_K         | 3.35                  |
| Q3_K_S       | 3.50                  |
| Q3_K_M       | 3.91                  |
| Q3_K_L       | 4.27                  |
| Q4_K_S       | 4.58                  |
| Q4_K_M       | 4.84                  |
| Q5_K_S       | 5.52                  |
| Q5_K_M       | 5.68                  |
| Q6_K         | 6.56                  |
