# llama.cpp

![llama](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)

[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Server](https://github.com/ggerganov/llama.cpp/actions/workflows/server.yml/badge.svg?branch=master&event=schedule)](https://github.com/ggerganov/llama.cpp/actions/workflows/server.yml)
[![Conan Center](https://shields.io/conan/v/llama-cpp)](https://conan.io/center/llama-cpp)

[è·¯çº¿å›¾](https://github.com/users/ggerganov/projects/7) / [é¡¹ç›®çŠ¶æ€](https://github.com/ggerganov/llama.cpp/discussions/3471) / [å®£è¨€](https://github.com/ggerganov/llama.cpp/discussions/205) / [ggml](https://github.com/ggerganov/ggml)

çº¯C/C++å®ç°Metaçš„[LLaMA](https://arxiv.org/abs/2302.13971)æ¨¡å‹ï¼ˆåŠå…¶ä»–æ¨¡å‹ï¼‰çš„æ¨ç†

> [!IMPORTANT]
[2024å¹´6æœˆ12æ—¥] äºŒè¿›åˆ¶æ–‡ä»¶å·²é‡å‘½åï¼Œå¸¦æœ‰`llama-`å‰ç¼€ã€‚`main`ç°åœ¨æ˜¯`llama-cli`ï¼Œ`server`æ˜¯`llama-server`ç­‰ï¼ˆhttps://github.com/ggerganov/llama.cpp/pull/7809ï¼‰

## è¿‘æœŸAPIå˜æ›´

- [2024å¹´6æœˆ26æ—¥] é‡æ–°ç»“æ„åŒ–æºä»£ç å’ŒCMakeæ„å»ºè„šæœ¬ https://github.com/ggerganov/llama.cpp/pull/8006
- [2024å¹´4æœˆ21æ—¥] `llama_token_to_piece` ç°åœ¨å¯ä»¥å¯é€‰åœ°æ¸²æŸ“ç‰¹æ®Šä»¤ç‰Œ https://github.com/ggerganov/llama.cpp/pull/6807
- [2024å¹´4æœˆ4æ—¥] å°†çŠ¶æ€å’Œä¼šè¯æ–‡ä»¶å‡½æ•°é‡æ–°ç»„ç»‡åˆ° `llama_state_*` ä¸‹ https://github.com/ggerganov/llama.cpp/pull/6341
- [2024å¹´3æœˆ26æ—¥] æ›´æ–°Logitså’ŒEmbeddings APIä»¥å‡å°ä½“ç§¯ https://github.com/ggerganov/llama.cpp/pull/6122
- [2024å¹´3æœˆ13æ—¥] æ·»åŠ  `llama_synchronize()` + `llama_context_params.n_ubatch` https://github.com/ggerganov/llama.cpp/pull/6017
- [2024å¹´3æœˆ8æ—¥] `llama_kv_cache_seq_rm()` è¿”å› `bool` è€Œä¸æ˜¯ `void`ï¼Œå¹¶ä¸”æ–°çš„ `llama_n_seq_max()` è¿”å›æ‰¹å¤„ç†ä¸­å¯æ¥å—çš„ `seq_id` çš„ä¸Šé™ï¼ˆå½“å¤„ç†å¤šä¸ªåºåˆ—æ—¶ç›¸å…³ï¼‰ https://github.com/ggerganov/llama.cpp/pull/5328
- [2024å¹´3æœˆ4æ—¥] æ›´æ–°Embeddings API https://github.com/ggerganov/llama.cpp/pull/5796
- [2024å¹´3æœˆ3æ—¥] `struct llama_context_params` https://github.com/ggerganov/llama.cpp/pull/5849

## çƒ­é—¨è¯é¢˜

- **`convert.py` å·²è¢«å¼ƒç”¨ï¼Œå¹¶ç§»åŠ¨åˆ° `examples/convert_legacy_llama.py`ï¼Œè¯·ä½¿ç”¨ `convert_hf_to_gguf.py`** https://github.com/ggerganov/llama.cpp/pull/7430
- åˆå§‹ Flash-Attention æ”¯æŒï¼šhttps://github.com/ggerganov/llama.cpp/pull/5021
- å·²æ·»åŠ  BPE é¢„æ ‡è®°æ”¯æŒï¼šhttps://github.com/ggerganov/llama.cpp/pull/6920
- MoE å†…å­˜å¸ƒå±€å·²æ›´æ–° - é‡æ–°è½¬æ¢æ¨¡å‹ä»¥æ”¯æŒ `mmap` å¹¶é‡æ–°ç”Ÿæˆ `imatrix` https://github.com/ggerganov/llama.cpp/pull/6387
- ä½¿ç”¨ `gguf-split` çš„æ¨¡å‹åˆ†ç‰‡æŒ‡ä»¤ https://github.com/ggerganov/llama.cpp/discussions/6404
- ä¿®å¤ Metal æ‰¹é‡æ¨ç†ä¸­çš„ä¸»è¦é”™è¯¯ https://github.com/ggerganov/llama.cpp/pull/6225
- æ”¯æŒ Multi-GPU ç®¡é“å¹¶è¡Œ https://github.com/ggerganov/llama.cpp/pull/6017
- å¯»æ‰¾è´¡çŒ®ä»¥æ·»åŠ  Deepseek æ”¯æŒï¼šhttps://github.com/ggerganov/llama.cpp/issues/5981
- å®šé‡ç›²æµ‹è¯•ï¼šhttps://github.com/ggerganov/llama.cpp/discussions/5962
- å·²æ·»åŠ åˆå§‹ Mamba æ”¯æŒï¼šhttps://github.com/ggerganov/llama.cpp/pull/5328

----

## æè¿°

`llama.cpp`çš„ä¸»è¦ç›®æ ‡æ˜¯å®ç°LLMæ¨ç†ï¼Œè®¾ç½®ç®€å•ä¸”åœ¨å¤šç§ç¡¬ä»¶ä¸Šï¼ˆæœ¬åœ°å’Œäº‘ç«¯ï¼‰æä¾›æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

- ä½¿ç”¨çº¯C/C++å®ç°ï¼Œæ— ä»»ä½•ä¾èµ–
- Appleç¡…èŠ¯ç‰‡æ˜¯é¦–é€‰å…¬æ°‘ - é€šè¿‡ARM NEONã€Accelerateå’ŒMetalæ¡†æ¶è¿›è¡Œä¼˜åŒ–
- æ”¯æŒx86æ¶æ„çš„AVXã€AVX2å’ŒAVX512
- æ”¯æŒç”¨äºå¿«é€Ÿæ¨ç†å’Œå‡å°‘å†…å­˜ä½¿ç”¨çš„1.5ä½ã€2ä½ã€3ä½ã€4ä½ã€5ä½ã€6ä½å’Œ8ä½æ•´æ•°é‡åŒ–
- ä¸ºåœ¨NVIDIA GPUä¸Šè¿è¡ŒLLMè‡ªå®šä¹‰CUDAå†…æ ¸ï¼ˆé€šè¿‡HIPæ”¯æŒAMD GPUï¼‰
- æ”¯æŒVulkanå’ŒSYCLåç«¯
- CPU+GPUæ··åˆæ¨ç†ä»¥éƒ¨åˆ†åŠ é€Ÿè¶…è¿‡æ€»VRAMå®¹é‡çš„æ¨¡å‹

è‡ªå…¶[å¯åŠ¨](https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022)ä»¥æ¥ï¼Œè¯¥é¡¹ç›®å¾—ç›Šäºè®¸å¤šè´¡çŒ®è€…çš„åŠªåŠ›ï¼Œå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å®ƒæ˜¯ä¸º[ggml](https://github.com/ggerganov/ggml)åº“å¼€å‘æ–°åŠŸèƒ½çš„ä¸»è¦å®éªŒåœºæ‰€ã€‚

**æ”¯æŒçš„æ¨¡å‹ï¼š**

é€šå¸¸æ”¯æŒä»¥ä¸‹åŸºç¡€æ¨¡å‹çš„å¾®è°ƒç‰ˆæœ¬ã€‚

- [X] LLaMA ğŸ¦™
- [x] LLaMA 2 ğŸ¦™ğŸ¦™
- [x] LLaMA 3 ğŸ¦™ğŸ¦™ğŸ¦™
- [X] [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)
- [x] [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)
- [x] [DBRX](https://huggingface.co/databricks/dbrx-instruct)
- [X] [Falcon](https://huggingface.co/models?search=tiiuae/falcon)
- [X] [Chinese LLaMA / Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) å’Œ [Chinese LLaMA-2 / Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)
- [X] [Vigogne (French)](https://github.com/bofenghuang/vigogne)
- [X] [BERT](https://github.com/ggerganov/llama.cpp/pull/5423)
- [X] [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)
- [X] [Baichuan 1 & 2](https://huggingface.co/models?search=baichuan-inc/Baichuan) + [è¡ç”Ÿ](https://huggingface.co/hiyouga/baichuan-7b-sft)
- [X] [Aquila 1 & 2](https://huggingface.co/models?search=BAAI/Aquila)
- [X] [Starcoder models](https://github.com/ggerganov/llama.cpp/pull/3187)
- [X] [Refact](https://huggingface.co/smallcloudai/Refact-1_6B-fim)

- [X] [MPT](https://github.com/ggerganov/llama.cpp/pull/3417)
- [X] [Bloom](https://github.com/ggerganov/llama.cpp/pull/3553)
- [x] [Yiæ¨¡å‹](https://huggingface.co/models?search=01-ai/Yi)
- [X] [StableLMæ¨¡å‹](https://huggingface.co/stabilityai)
- [x] [Deepseekæ¨¡å‹](https://huggingface.co/models?search=deepseek-ai/deepseek)
- [x] [Qwenæ¨¡å‹](https://huggingface.co/models?search=Qwen/Qwen)
- [x] [PLaMo-13B](https://github.com/ggerganov/llama.cpp/pull/3557)
- [x] [Phiæ¨¡å‹](https://huggingface.co/models?search=microsoft/phi)
- [x] [GPT-2](https://huggingface.co/gpt2)
- [x] [Orion 14B](https://github.com/ggerganov/llama.cpp/pull/5118)
- [x] [InternLM2](https://huggingface.co/models?search=internlm2)
- [x] [CodeShell](https://github.com/WisdomShell/codeshell)
- [x] [Gemma](https://ai.google.dev/gemma)
- [x] [Mamba](https://github.com/state-spaces/mamba)
- [x] [Grok-1](https://huggingface.co/keyfan/grok-1-hf)
- [x] [Xverse](https://huggingface.co/models?search=xverse)
- [x] [Command-Ræ¨¡å‹](https://huggingface.co/models?search=CohereForAI/c4ai-command-r)
- [x] [SEA-LION](https://huggingface.co/models?search=sea-lion)
- [x] [GritLM-7B](https://huggingface.co/GritLM/GritLM-7B) + [GritLM-8x7B](https://huggingface.co/GritLM/GritLM-8x7B)
- [x] [OLMo](https://allenai.org/olmo)
- [x] [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) + [Pythia](https://github.com/EleutherAI/pythia)

(æ”¯æŒæ›´å¤šæ¨¡å‹çš„è¯´æ˜ï¼š[å¦‚ä½•æ·»åŠ æ¨¡å‹.md](./docs/HOWTO-add-model.md))

**å¤šæ¨¡æ€æ¨¡å‹ï¼š**

- [x] [LLaVA 1.5æ¨¡å‹](https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e), [LLaVA 1.6æ¨¡å‹](https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2)
- [x] [BakLLaVA](https://huggingface.co/models?search=SkunkworksAI/Bakllava)
- [x] [Obsidian](https://huggingface.co/NousResearch/Obsidian-3B-V0.5)
- [x] [ShareGPT4V](https://huggingface.co/models?search=Lin-Chen/ShareGPT4V)
- [x] [MobileVLM 1.7B/3Bæ¨¡å‹](https://huggingface.co/models?search=mobileVLM)
- [x] [Yi-VL](https://huggingface.co/models?search=Yi-VL)
- [x] [Mini CPM](https://huggingface.co/models?search=MiniCPM)

- [x] [Moondream](https://huggingface.co/vikhyatk/moondream2)
- [x] [Bunny](https://github.com/BAAI-DCAI/Bunny)

**ç»‘å®šï¼š**

- Python: [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- Go: [go-skynet/go-llama.cpp](https://github.com/go-skynet/go-llama.cpp)
- Node.js: [withcatai/node-llama-cpp](https://github.com/withcatai/node-llama-cpp)
- JS/TS (llama.cpp æœåŠ¡å™¨å®¢æˆ·ç«¯): [lgrammel/modelfusion](https://modelfusion.dev/integration/model-provider/llamacpp)
- JavaScript/Wasm (åœ¨æµè§ˆå™¨ä¸­å·¥ä½œ): [tangledgroup/llama-cpp-wasm](https://github.com/tangledgroup/llama-cpp-wasm)
- Typescript/Wasm (æ›´å‹å¥½çš„APIï¼Œå¯åœ¨npmä¸Šä½¿ç”¨): [ngxson/wllama](https://github.com/ngxson/wllama)
- Ruby: [yoshoku/llama_cpp.rb](https://github.com/yoshoku/llama_cpp.rb)
- Rust (æ›´å¤šåŠŸèƒ½): [edgenai/llama_cpp-rs](https://github.com/edgenai/llama_cpp-rs)
- Rust (æ›´å‹å¥½çš„API): [mdrokz/rust-llama.cpp](https://github.com/mdrokz/rust-llama.cpp)
- Rust (æ›´ç›´æ¥çš„ç»‘å®š): [utilityai/llama-cpp-rs](https://github.com/utilityai/llama-cpp-rs)
- C#/.NET: [SciSharp/LLamaSharp](https://github.com/SciSharp/LLamaSharp)
- Scala 3: [donderom/llm4s](https://github.com/donderom/llm4s)
- Clojure: [phronmophobic/llama.clj](https://github.com/phronmophobic/llama.clj)
- React Native: [mybigday/llama.rn](https://github.com/mybigday/llama.rn)
- Java: [kherud/java-llama.cpp](https://github.com/kherud/java-llama.cpp)
- Zig: [deins/llama.cpp.zig](https://github.com/Deins/llama.cpp.zig)
- Flutter/Dart: [netdur/llama_cpp_dart](https://github.com/netdur/llama_cpp_dart)
- PHP (åŸºäºllama.cppçš„APIç»‘å®šå’ŒåŠŸèƒ½): [distantmagic/resonance](https://github.com/distantmagic/resonance) [(æ›´å¤šä¿¡æ¯)](https://github.com/ggerganov/llama.cpp/pull/6326)

**UI:**

é™¤éå¦æœ‰è¯´æ˜ï¼Œè¿™äº›é¡¹ç›®éƒ½é‡‡ç”¨å¼€æ”¾æºä»£ç è®¸å¯ï¼š

- [iohub/collama](https://github.com/iohub/coLLaMA)
- [janhq/jan](https://github.com/janhq/jan) (AGPL)
- [nat/openplayground](https://github.com/nat/openplayground)
- [Faraday](https://faraday.dev/) (ä¸“æœ‰)
- [LMStudio](https://lmstudio.ai/) (ä¸“æœ‰)
- [Layla](https://play.google.com/store/apps/details?id=com.laylalite) (ä¸“æœ‰)

- [LocalAI](https://github.com/mudler/LocalAI) (MIT)
- [LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp) (AGPL)
- [Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile)
- [nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all)
- [ollama/ollama](https://github.com/ollama/ollama)
- [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) (AGPL)
- [psugihara/FreeChat](https://github.com/psugihara/FreeChat)
- [cztomsik/ava](https://github.com/cztomsik/ava) (MIT)
- [ptsochantaris/emeltal](https://github.com/ptsochantaris/emeltal)
- [pythops/tenere](https://github.com/pythops/tenere) (AGPL)
- [RAGNA æ¡Œé¢ç‰ˆ](https://ragna.app/) (ç§æœ‰)
- [RecurseChat](https://recurse.chat/) (ç§æœ‰)
- [semperai/amica](https://github.com/semperai/amica)
- [withcatai/catai](https://github.com/withcatai/catai)
- [ç§»åŠ¨äººå·¥æ™ºèƒ½/maid](https://github.com/Mobile-Artificial-Intelligence/maid) (MIT)
- [Msty](https://msty.app) (ç§æœ‰)
- [LLMFarm](https://github.com/guinmoon/LLMFarm?tab=readme-ov-file) (MIT)
- [KanTV](https://github.com/zhouwg/kantv?tab=readme-ov-file)(Apachev2.0æˆ–æ›´é«˜ç‰ˆæœ¬)
- [Dot](https://github.com/alexpinel/Dot) (GPL)
- [MindMac](https://mindmac.app) (ç§æœ‰)
- [KodiBot](https://github.com/firatkiral/kodibot) (GPL)
- [eva](https://github.com/ylsdamxssjxxdd/eva) (MIT)
- [AI Sublime Textæ’ä»¶](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (MIT)
- [AIKit](https://github.com/sozercan/aikit) (MIT)
- [LARS - LLM & Advanced Referencing Solution](https://github.com/abgulati/LARS) (AGPL)

*(è¦åœ¨æ­¤å¤„åˆ—å‡ºé¡¹ç›®ï¼Œå®ƒåº”æ˜ç¡®å£°æ˜å®ƒä¾èµ–äº`llama.cpp`)*

**å·¥å…·ï¼š**

- [akx/ggify](https://github.com/akx/ggify) â€“ ä»HuggingFace Hubä¸‹è½½PyTorchæ¨¡å‹å¹¶å°†å…¶è½¬æ¢ä¸ºGGML
- [crashr/gppm](https://github.com/crashr/gppm) â€“ åˆ©ç”¨NVIDIA Tesla P40æˆ–P100 GPUå¯åŠ¨llama.cppå®ä¾‹ï¼Œä»¥é™ä½ç©ºé—²åŠŸç‡æ¶ˆè€—

**åŸºç¡€è®¾æ–½ï¼š**

- [Paddler](https://github.com/distantmagic/paddler) - å®šåˆ¶çš„æœ‰çŠ¶æ€è´Ÿè½½å‡è¡¡å™¨ï¼Œä¸“ä¸ºllama.cppè®¾è®¡

## ç¤ºä¾‹

<details>
<summary>ä½¿ç”¨LLaMA v2 13Båœ¨M2 Ultraä¸Šçš„å…¸å‹è¿è¡Œ</summary>

```bash
$ make -j && ./llama-cli -m models/llama-13b-v2/ggml-model-q4_0.gguf -p "å»ºç«‹ç½‘ç«™å¯ä»¥é€šè¿‡10ä¸ªç®€å•çš„æ­¥éª¤æ¥å®Œæˆ:\næ­¥éª¤1:" -n 400 -e
I llama.cpp build infoï¼š
I UNAME_S:  Darwin
I UNAME_P:  arm
I UNAME_M:  arm64
I CFLAGS:   -I.            -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -pthread -DGGML_USE_K_QUANTS -DGGML_USE_ACCELERATE
I CXXFLAGS: -I. -I./common -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -DGGML_USE_K_QUANTS
I LDFLAGS:   -framework Accelerate
I CC:       Apple clang version 14.0.3 (clang-1403.0.22.14.1)
I CXX:      Apple clang version 14.0.3 (clang-1403.0.22.14.1)

make: Nothing to be done for `default'ã€‚
main: build = 1041 (cf658ad)
main: seed  = 1692823051
llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from models/llama-13b-v2/ggml-model-q4_0.gguf (version GGUF V1 (latest))
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q4_0:  281 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_print_meta: format         = GGUF V1 (latest)
llm_load_print_meta: arch           = llama
llm_load_print_meta: vocab type     = SPM
llm_load_print_meta: n_vocab        = 32000
llm_load_print_meta: n_merges       = 0
llm_load_print_meta: n_ctx_train    = 4096
llm_load_print_meta: n_ctx          = 512
llm_load_print_meta: n_embd         = 5120
llm_load_print_meta: n_head         = 40
llm_load_print_meta: n_head_kv      = 40
llm_load_print_meta: n_layer        = 40
llm_load_print_meta: n_rot          = 128
llm_load_print_meta: n_gqa          = 1
llm_load_print_meta: f_norm_eps     = 1.0e-05
llm_load_print_meta: f_norm_rms_eps = 1.0e-05
llm_load_print_meta: n_ff           = 13824
llm_load_print_meta: freq_base      = 10000.0
llm_load_print_meta: freq_scale     = 1
llm_load_print_meta: model type     = 13B
llm_load_print_meta: model ftype    = mostly Q4_0
llm_load_print_meta: model size     = 13.02 B
llm_load_print_meta: general.name   = LLaMA v2
llm_load_print_meta: BOS token = 1 '<s>'
llm_load_print_meta: EOS token = 2 '</s>'
llm_load_print_meta: UNK token = 0 '<unk>'
llm_load_print_meta: LF token  = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.11 MB
llm_load_tensors: mem required  = 7024.01 MB (+  400.00 MB per state)
...................................................................................................
llama_new_context_with_model: kv self size  =  400.00 MB
llama_new_context_with_model: compute buffer total size =   75.41 MB

system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |
sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000
generate: n_ctx = 512, n_batch = 512, n_predict = 400, n_keep = 0


å»ºç«‹ä¸€ä¸ªç½‘ç«™å¯ä»¥é€šè¿‡10ä¸ªç®€å•çš„æ­¥éª¤å®Œæˆï¼š
æ­¥éª¤1: æ‰¾åˆ°åˆé€‚çš„ç½‘ç«™å¹³å°ã€‚
æ­¥éª¤2: é€‰æ‹©ä½ çš„åŸŸåå’Œæ‰˜ç®¡è®¡åˆ’ã€‚
æ­¥éª¤3: è®¾è®¡ä½ çš„ç½‘ç«™å¸ƒå±€ã€‚
æ­¥éª¤4: ç¼–å†™ä½ çš„ç½‘ç«™å†…å®¹å¹¶æ·»åŠ å›¾ç‰‡ã€‚
æ­¥éª¤5: å®‰è£…å®‰å…¨åŠŸèƒ½ä»¥ä¿æŠ¤ä½ çš„ç½‘ç«™å…å—é»‘å®¢æˆ–åƒåœ¾é‚®ä»¶å‘é€è€…çš„ä¾µå®³
æ­¥éª¤6: åœ¨å¤šä¸ªæµè§ˆå™¨ã€ç§»åŠ¨è®¾å¤‡ã€æ“ä½œç³»ç»Ÿç­‰ä¸Šè¿›è¡Œç½‘ç«™æµ‹è¯•
æ­¥éª¤7: è®©ä¸ä½ æ— å…³çš„äººå†æ¬¡æµ‹è¯•å®ƒâ€”â€”æœ‹å‹æˆ–å®¶äººå°±å¯ä»¥ï¼
æ­¥éª¤8: é€šè¿‡ç¤¾äº¤åª’ä½“æ¸ é“æˆ–ä»˜è´¹å¹¿å‘Šå¼€å§‹æ¨å¹¿å’Œè¥é”€ç½‘ç«™
æ­¥éª¤9: åˆ†æåˆ°ç›®å‰ä¸ºæ­¢æœ‰å¤šå°‘è®¿å®¢è®¿é—®äº†ä½ çš„ç½‘ç«™ï¼Œå“ªäº›ç±»å‹çš„è®¿å®¢è®¿é—®æ›´é¢‘ç¹ï¼ˆä¾‹å¦‚ï¼Œç”·æ€§ vs å¥³æ€§ï¼‰ç­‰...
æ­¥éª¤10: é€šè¿‡éµå¾ªç½‘é¡µè®¾è®¡è¶‹åŠ¿å¹¶ä¿æŒå¯¹æ–°æŠ€æœ¯çš„äº†è§£ï¼Œç»§ç»­æ”¹è¿›ä¸Šè¿°æ‰€æœ‰æ–¹é¢ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºç”¨æˆ·ä½“éªŒï¼
ç½‘ç«™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
ç½‘ç«™é€šè¿‡é¡µé¢ç»„æˆï¼Œè¿™äº›é¡µé¢ç”±HTMLä»£ç åˆ¶æˆã€‚æ­¤ä»£ç å‘Šè¯‰æ‚¨çš„è®¡ç®—æœºå¦‚ä½•æ˜¾ç¤ºæ‚¨è®¿é—®çš„æ¯ä¸ªé¡µé¢çš„å†…å®¹â€”â€”æ— è®ºæ˜¯å›¾åƒè¿˜æ˜¯æ–‡æœ¬æ–‡ä»¶ï¼ˆå¦‚PDFï¼‰ã€‚ä¸ºäº†è®©å…¶ä»–äººçš„æµè§ˆå™¨ä¸ä»…èƒ½å¤Ÿï¼Œè€Œä¸”æ„¿æ„åœ¨è®¿é—®ä»»ä½•ç»™å®šçš„URLæ—¶è·å¾—ç›¸åŒçš„ç»“æœï¼›éœ€è¦é€šè¿‡ç¼–ç¨‹è„šæœ¬é‡‡å–ä¸€äº›é¢å¤–çš„æ­¥éª¤ï¼Œè¿™äº›è„šæœ¬å°†æ·»åŠ åŠŸèƒ½ï¼Œä¾‹å¦‚ä½¿é“¾æ¥å¯ç‚¹å‡»ï¼

æœ€å¸¸è§çš„ä¸€ç§ç±»å‹è¢«ç§°ä¸ºé™æ€HTMLé¡µé¢ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ—¶é—´ä¸Šä¿æŒä¸å˜ï¼Œé™¤éæ‰‹åŠ¨ä¿®æ”¹ï¼ˆç›´æ¥ç¼–è¾‘æ–‡ä»¶æˆ–ä½¿ç”¨å¦‚WordPressè¿™æ ·çš„ç•Œé¢ï¼‰ã€‚å®ƒä»¬é€šå¸¸é€šè¿‡HTTPåè®®æä¾›æœåŠ¡ - è¿™æ„å‘³ç€ä»»ä½•äººéƒ½å¯ä»¥è®¿é—®å®ƒä»¬ï¼Œè€Œæ— éœ€ä»»ä½•ç‰¹æ®Šæƒé™ï¼Œä¾‹å¦‚æˆä¸ºå…è®¸è¿›å…¥åœ¨çº¿å—é™åŒºåŸŸçš„ç¾¤ä½“çš„ä¸€å‘˜ï¼›ç„¶è€Œï¼Œæ ¹æ®ä¸€ä¸ªäººå±…ä½çš„åœ°ç†ä½ç½®ï¼Œå¯èƒ½ä»ç„¶å­˜åœ¨ä¸€äº›é™åˆ¶ã€‚

å¦‚ä½•

llama_print_timings:        åŠ è½½æ—¶é—´ =   576.45 æ¯«ç§’
llama_print_timings:      æ ·æœ¬æ—¶é—´ =   283.10 æ¯«ç§’ /   400 æ¬¡   (    0.71 æ¯«ç§’æ¯ä»¤ç‰Œï¼Œ  1412.91 ä»¤ç‰Œæ¯ç§’)
llama_print_timings: prompt eval time =   599.83 æ¯«ç§’ /    19 ä»¤ç‰Œ (   31.57 æ¯«ç§’æ¯ä»¤ç‰Œï¼Œ    31.68 ä»¤ç‰Œæ¯ç§’)
llama_print_timings:        è¯„ä¼°æ—¶é—´ = 24513.59 æ¯«ç§’ /   399 æ¬¡   (   61.44 æ¯«ç§’æ¯ä»¤ç‰Œï¼Œ    16.28 ä»¤ç‰Œæ¯ç§’)
llama_print_timings:       æ€»æ—¶é—´ = 25431.49 æ¯«ç§’
```

</details>

<details>
<summary>åœ¨å•ä¸ªM1 Pro MacBookä¸Šè¿è¡ŒLLaMA-7Bå’Œwhisper.cppçš„æ¼”ç¤º</summary>

ä»¥ä¸‹æ˜¯å¦ä¸€ä¸ªåœ¨å•ä¸ªM1 Pro MacBookä¸Šè¿è¡ŒLLaMA-7Bå’Œ[whisper.cpp](https://github.com/ggerganov/whisper.cpp)çš„æ¼”ç¤ºï¼š

https://user-images.githubusercontent.com/1991296/224442907-7693d4be-acaa-4e01-8b4f-add84093ffff.mp4

</details>

## ä½¿ç”¨æ–¹æ³•

ä»¥ä¸‹æ˜¯å¤§å¤šæ•°æ”¯æŒæ¨¡å‹çš„ç«¯åˆ°ç«¯äºŒè¿›åˆ¶æ„å»ºå’Œæ¨¡å‹è½¬æ¢æ­¥éª¤ã€‚

### åŸºæœ¬ä½¿ç”¨

é¦–å…ˆï¼Œä½ éœ€è¦è·å–äºŒè¿›åˆ¶æ–‡ä»¶ã€‚ä½ å¯ä»¥éµå¾ªä¸åŒçš„æ–¹æ³•ï¼š
- æ–¹æ³•1ï¼šå…‹éš†æ­¤å­˜å‚¨åº“å¹¶åœ¨æœ¬åœ°æ„å»ºï¼Œè¯·å‚é˜…[å¦‚ä½•æ„å»º](./docs/build.md)
- æ–¹æ³•2ï¼šå¦‚æœä½ ä½¿ç”¨MacOSæˆ–Linuxï¼Œå¯ä»¥é€šè¿‡[brew, floxæˆ–nix](./docs/install.md)å®‰è£…llama.cpp
- æ–¹æ³•3ï¼šä½¿ç”¨Dockeré•œåƒï¼Œè¯·å‚é˜…[å…³äºDockerçš„æ–‡æ¡£](./docs/docker.md)
- æ–¹æ³•4ï¼šä»[å‘å¸ƒç‰ˆ](https://github.com/ggerganov/llama.cpp/releases)ä¸‹è½½é¢„æ„å»ºçš„äºŒè¿›åˆ¶æ–‡ä»¶

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œä¸€ä¸ªåŸºæœ¬çš„å®Œæˆæ“ä½œï¼š

```bash
llama-cli -m your_model.gguf -p "æˆ‘ç›¸ä¿¡ç”Ÿå‘½çš„æ„ä¹‰æ˜¯" -n 128

# è¾“å‡ºï¼š
# æˆ‘ç›¸ä¿¡ç”Ÿå‘½çš„æ„ä¹‰æ˜¯æ‰¾åˆ°ä½ è‡ªå·±çš„çœŸç†ï¼Œå¹¶æŒ‰ç…§å®ƒç”Ÿæ´»ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œè¿™æ„å‘³ç€å¯¹è‡ªå·±çœŸè¯šï¼Œè¿½éšæˆ‘çš„æ¿€æƒ…ï¼Œå³ä½¿å®ƒä»¬ä¸ç¤¾ä¼šæœŸæœ›ä¸ä¸€è‡´ã€‚æˆ‘è®¤ä¸ºè¿™å°±æ˜¯æˆ‘å–œæ¬¢ç‘œä¼½çš„åŸå› â€”â€”å®ƒä¸ä»…ä»…æ˜¯ä¸€ç§èº«ä½“ç»ƒä¹ ï¼Œä¹Ÿæ˜¯ä¸€ç§ç²¾ç¥ç»ƒä¹ ã€‚å®ƒæ˜¯å…³äºä¸è‡ªå·±è¿æ¥ï¼Œå€¾å¬ä½ å†…å¿ƒçš„å£°éŸ³ï¼Œå¹¶å°Šé‡ä½ ç‹¬ç‰¹çš„æ—…ç¨‹ã€‚
```

è¯·å‚é˜…[è¿™ä¸ªé¡µé¢](./examples/main/README.md)ä»¥è·å–å‚æ•°çš„å®Œæ•´åˆ—è¡¨ã€‚

### å¯¹è¯æ¨¡å¼

å¦‚æœæ‚¨æƒ³è¦ä¸€ä¸ªæ›´ç±»ä¼¼ChatGPTçš„ä½“éªŒï¼Œå¯ä»¥é€šè¿‡ä¼ é€’å‚æ•°`-cnv`æ¥è¿è¡Œå¯¹è¯æ¨¡å¼ï¼š

```bash
llama-cli -m your_model.gguf -p "You are a helpful assistant" -cnv

# è¾“å‡ºï¼š
# > hi, who are you?
# Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?
#
# > what is 1+1?
# Easy peasy! The answer to 1+1 is... 2!
```

é»˜è®¤æƒ…å†µä¸‹ï¼ŒèŠå¤©æ¨¡æ¿å°†ä»è¾“å…¥æ¨¡å‹ä¸­è·å–ã€‚å¦‚æœæ‚¨æƒ³ä½¿ç”¨å¦ä¸€ä¸ªèŠå¤©æ¨¡æ¿ï¼Œå¯ä»¥é€šè¿‡ä¼ é€’å‚æ•°`--chat-template NAME`æ¥å®ç°ã€‚è¯·æŸ¥çœ‹[æ”¯æŒçš„æ¨¡æ¿åˆ—è¡¨](https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template)

```bash
./llama-cli -m your_model.gguf -p "You are a helpful assistant" -cnv --chat-template chatml
```

æ‚¨è¿˜å¯ä»¥é€šè¿‡`in-prefix`ã€`in-suffix`å’Œ`reverse-prompt`å‚æ•°ä½¿ç”¨è‡ªå·±çš„æ¨¡æ¿ï¼š

```bash
./llama-cli -m your_model.gguf -p "You are a helpful assistant" -cnv --in-prefix 'ç”¨æˆ·ï¼š ' --reverse-prompt 'ç”¨æˆ·ï¼š'
```

### WebæœåŠ¡å™¨

[llama.cpp WebæœåŠ¡å™¨](./examples/server/README.md)æ˜¯ä¸€ä¸ªè½»é‡çº§çš„HTTPæœåŠ¡å™¨ï¼Œå…¼å®¹[OpenAI API](https://github.com/openai/openai-openapi)ï¼Œå¯ä»¥ç”¨äºæä¾›æœ¬åœ°æ¨¡å‹ï¼Œå¹¶è½»æ¾åœ°å°†å®ƒä»¬è¿æ¥åˆ°ç°æœ‰çš„å®¢æˆ·ç«¯ã€‚

ä½¿ç”¨ç¤ºä¾‹ï¼š

```bash
./llama-server -m your_model.gguf --port 8080

# åŸºæœ¬Webç•Œé¢å¯ä»¥é€šè¿‡æµè§ˆå™¨è®¿é—®ï¼šhttp://localhost:8080
# èŠå¤©å®Œæˆç«¯ç‚¹ï¼šhttp://localhost:8080/v1/chat/completions
```

### äº¤äº’æ¨¡å¼

> [!æ³¨æ„]
> å¦‚æœæ‚¨æ›´å–œæ¬¢åŸºæœ¬ä½¿ç”¨ï¼Œè¯·è€ƒè™‘ä½¿ç”¨ä¼šè¯æ¨¡å¼è€Œä¸æ˜¯äº¤äº’æ¨¡å¼

åœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œæ‚¨å¯ä»¥é€šè¿‡æŒ‰Ctrl+Cæ¥ä¸­æ–­ç”Ÿæˆï¼Œå¹¶è¾“å…¥ä¸€è¡Œæˆ–å¤šè¡Œæ–‡æœ¬ï¼Œè¿™äº›æ–‡æœ¬å°†è¢«è½¬æ¢ä¸ºä»¤ç‰Œå¹¶é™„åŠ åˆ°å½“å‰ä¸Šä¸‹æ–‡ä¸­ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨å‚æ•°`-r "reverse prompt string"`æŒ‡å®šä¸€ä¸ª*åå‘æç¤º*ã€‚è¿™å°†åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é‡åˆ°ä¸åå‘æç¤ºå­—ç¬¦ä¸²å®Œå…¨ç›¸åŒçš„ä»¤ç‰Œæ—¶æç¤ºç”¨æˆ·è¾“å…¥ã€‚å…¸å‹ç”¨é€”æ˜¯ä½¿ç”¨ä¸€ä¸ªæç¤ºï¼Œä½¿LLaMAæ¨¡æ‹ŸAliceå’ŒBobç­‰å¤šäººä¹‹é—´çš„èŠå¤©ï¼Œä¾‹å¦‚ï¼Œä¼ é€’`-r "Alice:"`ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå‡ ä¸ªæ­¥éª¤çš„äº¤äº’ç¤ºä¾‹ï¼Œé€šè¿‡å‘½ä»¤è°ƒç”¨ï¼š

```bash
# ä½¿ç”¨7Bæ¨¡å‹é»˜è®¤å‚æ•°
./examples/chat.sh

# ä½¿ç”¨13Bæ¨¡å‹çš„é«˜çº§èŠå¤©
./examples/chat-13B.sh

# ä½¿ç”¨13Bæ¨¡å‹çš„è‡ªå®šä¹‰å‚æ•°
./llama-cli -m ./models/13B/ggml-model-q4_0.gguf -n 256 --repeat_penalty 1.0 --color -i -r "User:" -f prompts/chat-with-bob.txt
```

æ³¨æ„ä½¿ç”¨`--color`å‚æ•°æ¥åŒºåˆ†ç”¨æˆ·è¾“å…¥å’Œç”Ÿæˆçš„æ–‡æœ¬ã€‚å…¶ä»–å‚æ•°åœ¨`llama-cli`ç¤ºä¾‹ç¨‹åºçš„[README](examples/main/README.md)ä¸­æœ‰æ›´è¯¦ç»†çš„è§£é‡Šã€‚

![image](https://user-images.githubusercontent.com/1991296/224575029-2af3c7dc-5a65-4f64-a6bb-517a532aea38.png)

### æŒä¹…äº¤äº’

é€šè¿‡ä½¿ç”¨ `--prompt-cache` å’Œ `--prompt-cache-all`ï¼Œ`./llama-cli` è°ƒç”¨çš„æç¤ºã€ç”¨æˆ·è¾“å…¥å’Œæ¨¡å‹ç”Ÿæˆå¯ä»¥è¢«ä¿å­˜å¹¶é‡æ–°å¯åŠ¨ã€‚`./examples/chat-persistent.sh` è„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ”¯æŒé•¿æ—¶é—´è¿è¡Œã€å¯æ¢å¤çš„èŠå¤©ä¼šè¯ã€‚è¦ä½¿ç”¨æ­¤ç¤ºä¾‹ï¼Œæ‚¨å¿…é¡»æä¾›ä¸€ä¸ªç”¨äºç¼“å­˜åˆå§‹èŠå¤©æç¤ºçš„æ–‡ä»¶å’Œä¸€ä¸ªç”¨äºä¿å­˜èŠå¤©ä¼šè¯çš„ç›®å½•ï¼Œå¹¶ä¸”å¯ä»¥å¯é€‰åœ°æä¾›ä¸ `chat-13B.sh` ç›¸åŒçš„å˜é‡ã€‚ç›¸åŒçš„æç¤ºç¼“å­˜å¯ä»¥ç”¨äºæ–°çš„èŠå¤©ä¼šè¯ã€‚è¯·æ³¨æ„ï¼Œæç¤ºç¼“å­˜å’ŒèŠå¤©ç›®å½•éƒ½ä¸åˆå§‹æç¤º (`PROMPT_TEMPLATE`) å’Œæ¨¡å‹æ–‡ä»¶ç›¸å…³è”ã€‚

```bash
# å¼€å§‹ä¸€ä¸ªæ–°çš„èŠå¤©
PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/default ./examples/chat-persistent.sh

# é‡æ–°å¯åŠ¨é‚£ä¸ªèŠå¤©
PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/default ./examples/chat-persistent.sh

# ä½¿ç”¨ç›¸åŒçš„æç¤º/æ¨¡å‹å¼€å§‹å¦ä¸€ä¸ªèŠå¤©
PROMPT_CACHE_FILE=chat.prompt.bin CHAT_SAVE_DIR=./chat/another ./examples/chat-persistent.sh

# ä¸åŒæç¤º/æ¨¡å‹çš„ä¸åŒæç¤ºç¼“å­˜
PROMPT_TEMPLATE=./prompts/chat-with-bob.txt PROMPT_CACHE_FILE=bob.prompt.bin \
    CHAT_SAVE_DIR=./chat/bob ./examples/chat-persistent.sh
```

### ä½¿ç”¨è¯­æ³•é™åˆ¶è¾“å‡º

`llama.cpp` æ”¯æŒè¯­æ³•æ¥é™åˆ¶æ¨¡å‹è¾“å‡ºã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥å¼ºåˆ¶æ¨¡å‹ä»…è¾“å‡ºJSONï¼š

```bash
./llama-cli -m ./models/13B/ggml-model-q4_0.gguf -n 256 --grammar-file grammars/json.gbnf -p 'Request: schedule a call at 8pm; Command:'
```

`grammars/` æ–‡ä»¶å¤¹åŒ…å«äº†ä¸€äº›ç¤ºä¾‹è¯­æ³•ã€‚è¦ç¼–å†™è‡ªå·±çš„è¯­æ³•ï¼Œè¯·æŸ¥çœ‹[GBNFæŒ‡å—](./grammars/README.md)ã€‚

å¯¹äºç¼–å†™æ›´å¤æ‚çš„JSONè¯­æ³•ï¼Œæ‚¨è¿˜å¯ä»¥æŸ¥çœ‹ https://grammar.intrinsiclabs.ai/ï¼Œè¿™æ˜¯ä¸€ä¸ªæµè§ˆå™¨åº”ç”¨ç¨‹åºï¼Œå…è®¸æ‚¨ç¼–å†™TypeScriptæ¥å£ï¼Œå®ƒå°†ç¼–è¯‘æˆGBNFè¯­æ³•ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬ä¿å­˜ä»¥ä¾›æœ¬åœ°ä½¿ç”¨ã€‚è¯·æ³¨æ„ï¼Œè¯¥åº”ç”¨ç¨‹åºæ˜¯ç”±ç¤¾åŒºæˆå‘˜æ„å»ºå’Œç»´æŠ¤çš„ï¼Œè¯·åœ¨æ­¤ä»“åº“[æäº¤ä»»ä½•é—®é¢˜æˆ–åŠŸèƒ½è¯·æ±‚](http://github.com/intrinsiclabsai/gbnfgen)ã€‚

## æ„å»º

è¯·å‚é˜…[æœ¬åœ°æ„å»º llama.cpp](./docs/build.md)

## æ”¯æŒçš„åç«¯

| åç«¯ | ç›®æ ‡è®¾å¤‡ |
| --- | --- |
| [Metal](./docs/build.md#metal-build) | Apple Silicon |
| [BLAS](./docs/build.md#blas-build) | æ‰€æœ‰ |
| [BLIS](./docs/backend/BLIS.md) | æ‰€æœ‰ |
| [SYCL](./docs/backend/SYCL.md) | Intel å’Œ Nvidia GPU |
| [CUDA](./docs/build.md#cuda) | Nvidia GPU |
| [hipBLAS](./docs/build.md#hipblas) | AMD GPU |
| [Vulkan](./docs/build.md#vulkan) | GPU |

## å·¥å…·

### å‡†å¤‡å’Œé‡åŒ–

> [!NOTE]
> æ‚¨å¯ä»¥ä½¿ç”¨ Hugging Face ä¸Šçš„ [GGUF-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) ç©ºé—´å¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–ï¼Œæ— éœ€ä»»ä½•è®¾ç½®ã€‚å®ƒä¼šæ¯6å°æ—¶ä» `llama.cpp` ä¸»æ–‡ä»¶åŒæ­¥ä¸€æ¬¡ã€‚

è¦è·å–å®˜æ–¹ LLaMA 2 æƒé‡ï¼Œè¯·å‚é˜… <a href="#obtaining-and-using-the-facebook-llama-2-model">è·å–å’Œä½¿ç”¨ Facebook LLaMA 2 æ¨¡å‹</a> éƒ¨åˆ†ã€‚Hugging Face ä¸Šè¿˜æä¾›äº†å¤§é‡é¢„é‡åŒ–çš„ `gguf` æ¨¡å‹ã€‚

æ³¨æ„ï¼š`convert.py` å·²ç§»åŠ¨åˆ° `examples/convert_legacy_llama.py`ï¼Œå¹¶ä¸”ä»…åº”ç”¨äº `Llama/Llama2/Mistral` æ¨¡å‹åŠå…¶æ´¾ç”Ÿæ¨¡å‹ã€‚å®ƒä¸æ”¯æŒ LLaMA 3ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä» Hugging Face ä¸‹è½½çš„ LLaMA 3 ä½¿ç”¨ `convert_hf_to_gguf.py`ã€‚

è¦äº†è§£æ›´å¤šå…³äºé‡åŒ–æ¨¡å‹çš„ä¿¡æ¯ï¼Œ[è¯·é˜…è¯»æ­¤æ–‡æ¡£](./examples/quantize/README.md)

### éš¾åº¦ (è¡¡é‡æ¨¡å‹è´¨é‡)

æ‚¨å¯ä»¥ä½¿ç”¨ `perplexity` ç¤ºä¾‹æ¥æµ‹é‡ç»™å®šæç¤ºçš„éš¾åº¦ï¼ˆè¾ƒä½çš„éš¾åº¦æ›´å¥½ï¼‰ã€‚
æ›´å¤šä¿¡æ¯è¯·å‚é˜… [https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity)ã€‚

è¦äº†è§£æ›´å¤šå¦‚ä½•ä½¿ç”¨ llama.cpp æµ‹é‡éš¾åº¦ï¼Œ[è¯·é˜…è¯»æ­¤æ–‡æ¡£](./examples/perplexity/README.md)

## è´¡çŒ®

- è´¡çŒ®è€…å¯ä»¥æ‰“å¼€Pull Request (PR)
- åä½œè€…å¯ä»¥å‘`llama.cpp`ä»“åº“çš„åˆ†æ”¯æ¨é€æ›´æ”¹ï¼Œå¹¶å°†PRåˆå¹¶åˆ°`master`åˆ†æ”¯
- åä½œè€…å°†æ ¹æ®å…¶è´¡çŒ®è¢«é‚€è¯·
- å¯¹äºç®¡ç†å’Œå¤„ç†é—®é¢˜å’ŒPRçš„ä»»ä½•å¸®åŠ©éƒ½éå¸¸æ„Ÿæ¿€ï¼
- æŸ¥çœ‹é€‚åˆé¦–æ¬¡è´¡çŒ®çš„ä»»åŠ¡ [good first issues](https://github.com/ggerganov/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)
- è¯»å–[CONTRIBUTING.md](CONTRIBUTING.md)ä»¥è·å–æ›´å¤šä¿¡æ¯
- ä¸€å®šè¦é˜…è¯»è¿™ä¸ªï¼š[è¾¹ç¼˜æ¨ç†](https://github.com/ggerganov/llama.cpp/discussions/205)
- ä¸€äº›èƒŒæ™¯ä¿¡æ¯ä¾›æœ‰å…´è¶£çš„äººå‚è€ƒï¼š[å˜æ›´æ—¥å¿—æ’­å®¢](https://changelog.com/podcast/532)

## å…¶ä»–æ–‡æ¡£

- [main (å‘½ä»¤è¡Œ)](./examples/main/README.md)
- [æœåŠ¡å™¨](./examples/server/README.md)
- [Jeopardy](./examples/jeopardy/README.md)
- [GBNF è¯­æ³•](./grammars/README.md)

**å¼€å‘æ–‡æ¡£**

- [å¦‚ä½•æ„å»º](./docs/build.md)
- [åœ¨Dockerä¸Šè¿è¡Œ](./docs/docker.md)
- [åœ¨Androidä¸Šæ„å»º](./docs/android.md)
- [æ€§èƒ½æ•…éšœæ’é™¤](./docs/token_generation_performance_tips.md)
- [GGMLæŠ€å·§ä¸çªé—¨](https://github.com/ggerganov/llama.cpp/wiki/GGML-Tips-&-Tricks)

**å…³é”®è®ºæ–‡å’Œæ¨¡å‹èƒŒæ™¯**

å¦‚æœä½ çš„é—®é¢˜æ˜¯å…³äºæ¨¡å‹ç”Ÿæˆè´¨é‡ï¼Œé‚£ä¹ˆè¯·è‡³å°‘æ‰«æä»¥ä¸‹é“¾æ¥å’Œè®ºæ–‡ï¼Œäº†è§£LLaMAæ¨¡å‹çš„å±€é™æ€§ã€‚è¿™å¯¹äºé€‰æ‹©é€‚å½“æ¨¡å‹å¤§å°ï¼Œå¹¶æ¬£èµLLaMAæ¨¡å‹ä¸ChatGPTä¹‹é—´çš„æ˜¾è‘—å’Œç»†å¾®å·®å¼‚å°¤ä¸ºé‡è¦ï¼š

- LLaMAï¼š
    - [ä»‹ç»LLaMAï¼šä¸€ä¸ªåŸºç¡€æ€§ã€65äº¿å‚æ•°çš„å¤§è¯­è¨€æ¨¡å‹](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    - [LLaMAï¼šå¼€æ”¾ä¸”é«˜æ•ˆçš„åŸºçŸ³è¯­è¨€æ¨¡å‹](https://arxiv.org/abs/2302.13971)
- GPT-3
    - [è¯­è¨€æ¨¡å‹æ˜¯é›¶æ ·æœ¬å­¦ä¹ è€…](https://arxiv.org/abs/2005.14165)
- GPT-3.5 / InstructGPT / ChatGPTï¼š
    - [å¯¹é½è¯­è¨€æ¨¡å‹ä»¥éµå¾ªæŒ‡ä»¤](https://openai.com/research/instruction-following)
    - [ä½¿ç”¨äººç±»åé¦ˆè®­ç»ƒè¯­è¨€æ¨¡å‹ä»¥éµå¾ªæŒ‡ä»¤](https://arxiv.org/abs/2203.02155)

