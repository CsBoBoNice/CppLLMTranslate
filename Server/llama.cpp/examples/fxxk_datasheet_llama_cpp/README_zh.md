# llama.cpp/example/main

本示例程序允许您以简单高效的方式使用各种 LLaMA 语言模型。它专门设计用于与 [llama.cpp](https://github.com/ggerganov/llama.cpp) 项目一起工作，该项目提供了一个 plain C/C++ 实现，并可选支持 4 位量化，以实现更快的速度和更低的内存推理，并且针对桌面 CPU 进行了优化。此程序可以用于使用 LLaMA 模型执行各种推理任务，包括根据用户提供的提示生成文本，以及与反向提示进行聊天式交互。

## 目录

1. [快速开始](#quick-start)
2. [常用选项](#common-options)
3. [输入提示](#input-prompts)
4. [交互](#interaction)
5. [上下文管理](#context-management)
6. [生成标志](#generation-flags)
7. [性能调优和内存选项](#performance-tuning-and-memory-options)
8. [附加选项](#additional-options)

## 快速开始

要立即开始，请运行以下命令，确保使用正确的模型路径：


#### 基于 Unix 的系统 (Linux, macOS 等):

```bash
./llama-cli -m models/7B/ggml-model.bin --prompt "Once upon a time"
```

#### Windows:

```powershell
llama-cli.exe -m models\7B\ggml-model.bin --prompt "Once upon a time"
```

为了获得交互式体验，请尝试以下命令：

#### 基于 Unix 的系统 (Linux, macOS 等):

```bash
./llama-cli -m models/7B/ggml-model.bin -n -1 --color -r "User:" --in-prefix " " -i -p \
'User: Hi
AI: Hello. I am an AI chatbot. Would you like to talk?
User: Sure!
AI: What would you like to talk about?
User:'
```

#### Windows:

```powershell
llama-cli.exe -m models\7B\ggml-model.bin -n -1 --color -r "用户:" --in-prefix " " -i -e -p "用户: 嘿\nAI: 你好。我是一个 AI 聊天机器人。你想和我聊天吗？\n用户: 当然！\nAI: 你想聊些什么呢？\n用户:"
```

以下命令从起始提示中生成“无限”文本（你可以使用 `Ctrl-C` 来停止它）：

#### 基于 Unix 的系统 (Linux, macOS 等):

```bash
./llama-cli -m models/7B/ggml-model.bin --ignore-eos -n -1
```

#### Windows:

```powershell
llama-cli.exe -m models\7B\ggml-model.bin --ignore-eos -n -1
```

## 常见选项

在本节中，我们将介绍运行带有 LLaMA 模型的 `llama-cli` 程序时最常用的选项：

-   `-m FNAME, --model FNAME`: 指定 LLaMA 模型文件的路径（例如，`models/7B/ggml-model.gguf`；如果设置了 `--model-url`，则从中推断）。
-   `-mu MODEL_URL --model-url MODEL_URL`: 指定远程 http url 下载文件（例如 https://huggingface.co/ggml-org/models/resolve/main/phi-2/ggml-model-q4_0.gguf）。
-   `-i, --interactive`: 以交互模式运行程序，允许您直接提供输入并接收实时响应。
-   `-n N, --n-predict N`: 设置生成文本时要预测的标记数。调整此值可以影响生成文本的长度。
-   `-c N, --ctx-size N`: 设置提示上下文的大小。默认为 512，但 LLaMA 模型是在 2048 个上下文中构建的，这将提供更好的结果，尤其是对于较长的输入/推理。

## 输入提示

`llama-cli`程序提供多种方式与LLaMA模型进行交互，使用输入提示：

-   `--prompt PROMPT`: 直接作为命令行选项提供提示。
-   `--file FNAME`: 提供一个包含提示或多个提示的文件。
-   `--interactive-first`: 以交互模式运行程序并立即等待输入。(更多内容请参阅下文。)

## 交互

`llama-cli`程序提供了一种无缝的方式与LLaMA模型进行交互，使用户能够参与实时对话或为特定任务提供指令。交互模式可以通过多种选项触发，包括`--interactive`和`--interactive-first`。

在交互模式下，用户可以在过程中注入他们的输入来参与文本生成。用户可以在任何时候按`Ctrl+C`来中断并输入他们的输入，然后按`Return`将其提交给LLaMA模型。要提交额外的行而不最终确定输入，用户可以在当前行末尾加上反斜杠（\）并继续输入。

### 交互选项

-   `-i, --interactive`: 以交互模式运行程序，允许用户进行实时对话或向模型提供特定指令。
-   `--interactive-first`: 以交互模式运行程序，并在开始文本生成之前立即等待用户输入。
-   `--color`: 启用彩色输出，以便在视觉上区分提示、用户输入和生成的文本。

通过理解和利用这些交互选项，您可以使用 LLaMA 模型创建引人入胜和动态的体验，将文本生成过程定制到您的特定需求。

### 反向提示

反向提示是一种通过在遇到特定文本字符串时暂停文本生成来创建类似聊天体验的强大方式：

-   `-r PROMPT, --reverse-prompt PROMPT`: 指定一个或多个反向提示以暂停文本生成并切换到交互模式。例如，`-r "User:"` 可以用于在轮到用户发言时跳回对话。这有助于创建更具交互性和对话性的体验。然而，当反向提示以空格结尾时，它不会工作。

为了克服这个限制，您可以使用 `--in-prefix` 标志在反向提示后添加空格或任何其他字符。

### 前缀

`--in-prefix` 标志用于向您的输入添加前缀，主要用途是在反提示后插入一个空格。以下是如何结合使用 `--in-prefix` 和 `--reverse-prompt` 标志的示例：

```sh
./llama-cli -r "用户：" --in-prefix " "
```

### 后缀

`--in-suffix` 标志用于在您的输入后添加后缀。这很有用，例如在用户的输入后添加一个 "助手：" 提示。它添加在自动添加到用户输入末尾的新行字符 (`\n`) 之后。以下是如何结合使用 `--in-suffix` 和 `--reverse-prompt` 标志的示例：

```sh
./llama-cli -r "用户：" --in-prefix " " --in-suffix "助手："
```

## 上下文管理

在文本生成过程中，LLaMA 模型有一个有限的范围大小，这意味着它们只能考虑输入和生成文本中的特定数量的标记。当上下文填满时，模型会内部重置，可能会丢失一些来自对话或指令开始的信息。上下文管理选项有助于在这些情况下维持连续性和连贯性。

### 上下文大小

`--ctx-size` 选项允许您设置LLaMA模型在文本生成过程中使用的提示上下文的大小。更大的上下文大小有助于模型更好地理解并生成较长的输入或对话的响应。

-   `-c N, --ctx-size N`: 设置提示上下文的大小（默认：512）。LLaMA模型是使用2048个上下文构建的，这将产生较长的输入/推理的最佳结果。然而，将上下文大小增加到2048以上可能会导致不可预测的结果。

### 扩展上下文大小

一些经过微调的模型通过缩放RoPE来扩展上下文长度。例如，如果原始预训练模型的上下文长度（最大序列长度）为4096（4k），而微调模型为32k。这意味着缩放因子为8，应该通过将上述`--ctx-size`设置为32768（32k）和`--rope-scale`设置为8来实现。

-   `--rope-scale N`: 其中N是微调模型使用的线性缩放因子。

### 保持提示

`--keep` 选项允许用户在模型耗尽上下文时保留原始提示，确保与初始指令或对话主题的连接得到维持。

-   `--keep N`: 指定在模型重置其内部上下文时保留的初始提示中的令牌数量。默认情况下，此值设置为0（表示不保留任何令牌）。使用`-1`以保留初始提示中的所有令牌。

通过利用上下文管理选项如`--ctx-size`和`--keep`，您可以与LLaMA模型保持更连贯和一致的交互，确保生成的文本与原始提示或对话相关。

## 生成标志

以下选项允许您控制文本生成过程，并根据您的需求微调生成文本的多样性、创造性和质量。通过调整这些选项并尝试不同的值组合，您可以找到最适合您特定用例的最佳设置。

### 预测的令牌数量

-   `-n N, --n-predict N`: 设置在生成文本时预测的令牌数量（默认：128，-1 = 无限，-2 = 直到上下文填满）

`--n-predict` 选项控制模型对输入提示生成令牌的数量。通过调整此值，您可以影响生成文本的长度。更高的值会导致更长的文本，而较低的值会产生较短的文本。

值为 -1 将启用无限文本生成，尽管我们有一个有限上下文窗口。当上下文窗口满时，一些较早的令牌（`--n-keep` 后面的令牌的一半）将被丢弃。然后必须重新评估上下文，然后才能继续生成。在大型模型和/或大型上下文窗口的情况下，这将导致输出显著暂停。

如果不需要暂停，则值为 -2 将在上下文填满时立即停止生成。

需要注意的是，如果遇到结束序列 (EOS) 令牌或反向提示，生成的文本可能比指定的令牌数量短。在交互模式下，文本生成将暂停，并将控制权交还给用户。在非交互模式下，程序将结束。在两种情况下，文本生成可能停止在达到指定的 `n-predict` 值之前。如果您希望模型在没有产生结束序列的情况下继续进行，可以使用 `--ignore-eos` 参数。

### 温度

-   `--temp N`: 调整生成的文本的随机性（默认：0.8）。

温度是一个超参数，它控制生成文本的随机性。它影响模型输出标记的概率分布。较高的温度（例如，1.5）会使输出更加随机和富有创意，而较低的温度（例如，0.5）会使输出更加集中、确定和保守。默认值为0.8，在随机性和确定性之间提供了平衡。在极端情况下，温度为0将始终选择最可能的下一个标记，导致每次运行的结果都相同。

示例用法：`--temp 0.5`

### 重复惩罚

-   `--repeat-penalty N`: 控制生成的文本中标记序列的重复（默认：1.1）。
-   `--repeat-last-n N`: 考虑惩罚重复的最后一个n个标记（默认：64，0 = 禁用，-1 = ctx-size）。
-   `--no-penalize-nl`: 在应用重复惩罚时禁用换行符的惩罚。

`repeat-penalty`选项有助于防止模型生成重复或单调的文本。较高的值（例如，1.5）会对重复进行更强的惩罚，而较低的值（例如，0.9）则较为宽容。默认值为1.1。

`repeat-last-n`选项控制用于惩罚重复的历史标记数量。较大的值将在生成的文本中向后查看更远，以防止重复，而较小的值仅考虑最近的标记。值为0禁用惩罚，值为-1将考虑的标记数设置为上下文大小（`ctx-size`）。

使用`--no-penalize-nl`选项禁用在应用重复惩罚时的换行符惩罚。此选项特别适用于生成聊天对话、对话、代码、诗歌或任何文本，其中换行符在结构和格式中起着重要作用。禁用换行符惩罚有助于在特定用例中保持自然流动和预期的格式。

示例用法：`--repeat-penalty 1.15 --repeat-last-n 128 --no-penalize-nl`

### Top-K采样

-   `--top-k N`: 将下一个标记的选择限制为模型预测的最可能的K个标记（默认值：40）。

Top-k采样是一种文本生成方法，它只从模型预测的最可能的k个标记中选择下一个标记。这有助于减少生成低概率或不合逻辑的标记的风险，但这也可能限制了输出的多样性。Top-k的值越高（例如，100），考虑的标记就越多，生成的文本就会更加多样化；而Top-k的值越低（例如，10），就会更加关注最可能的标记，生成的文本就会更加保守。默认值为40。

示例用法：`--top-k 30`

### Top-P采样

-   `--top-p N`: 将下一个标记的选择限制为累积概率超过阈值P的标记子集（默认值：0.9）。

Top-p采样，也称为核采样，是另一种文本生成方法，它从累积概率至少为p的标记子集中选择下一个标记。这种方法通过考虑标记的概率以及从哪些标记中进行采样来平衡多样性和质量。Top-p的值越高（例如，0.95），生成的文本就会更加多样化；而Top-p的值越低（例如，0.5），生成的文本就会更加聚焦和保守。默认值为0.9。

示例用法：`--top-p 0.95`

### 最小P采样

-   `--min-p N`: 设置token选择的最低基础概率阈值（默认：0.05）。

最小P采样方法是为了作为Top-P的替代而设计的，旨在确保质量和多样性的平衡。参数*p*代表token被认为的最小概率，相对于最可能token的概率。例如，*p*=0.05且最可能token的概率为0.9时，小于0.045的logits会被过滤掉。

示例用法：`--min-p 0.05`

### 尾部自由采样（TFS）

-   `--tfs N`: 启用尾部自由采样，参数为z（默认：1.0，1.0表示禁用）。

尾部自由采样（TFS）是一种文本生成技术，旨在减少不太可能的token（可能不太相关、不太连贯或不合逻辑）对输出结果的影响。与Top-P类似，它试图动态确定最可能token的大部分。但TFS根据token概率的二阶导数过滤logits。在二阶导数之和达到参数z之前，停止添加token。简而言之：TFS查看token概率下降的速度，并使用参数z切断不太可能token的尾部。z的典型值在0.9到0.95的范围内。1.0的值会包括所有token，因此禁用了TFS的效果。

示例用法：`--tfs 0.95`

### 本地典型采样

-   `--typical N`: 启用本地典型采样，参数为p（默认：1.0，1.0 = 禁用）。

本地典型采样通过采样基于周围上下文的典型或预期标记，促进生成上下文一致且多样化的文本。通过设置参数p在0到1之间，您可以控制生成本地一致性和多样性之间的平衡。值越接近1，将促进更多上下文一致的标记，而值越接近0，将促进更多多样化的标记。值为1将禁用本地典型采样。

示例用法：`--typical 0.9`

### Mirostat采样

-   `--mirostat N`: 启用Mirostat采样，控制文本生成时的困惑度（默认：0，0 = 禁用，1 = Mirostat，2 = Mirostat 2.0）。
-   `--mirostat-lr N`: 设置Mirostat学习率，参数eta（默认：0.1）。
-   `--mirostat-ent N`: 设置Mirostat目标熵，参数tau（默认：5.0）。

Mirostat是一种算法，在文本生成过程中主动维护生成文本的质量在期望范围内。它旨在在一致性和多样性之间取得平衡，避免由于过度重复（无聊陷阱）或不一致（困惑陷阱）而产生的低质量输出。

`--mirostat-lr`选项设置Mirostat学习率（eta）。学习率影响算法对生成文本反馈的响应速度。较低的学习率会导致调整较慢，而较高的学习率会使算法更灵敏。默认值为`0.1`。

`--mirostat-ent`选项设置Mirostat目标熵（tau），代表生成文本期望的困惑度值。调整目标熵可以控制生成文本的一致性和多样性之间的平衡。较低的值会导致更集中和一致的文本，而较高的值会导致更多样化且可能不那么一致的文本。默认值为`5.0`。

示例用法：`--mirostat 2 --mirostat-lr 0.05 --mirostat-ent 3.0`

### Logit偏差

- `-l TOKEN_ID(+/-)BIAS, --logit-bias TOKEN_ID(+/-)BIAS`: 修改生成文本中某个标记出现的概率。

Logit偏差选项允许您手动调整特定标记出现在生成文本中的可能性。通过提供一个标记ID和正或负偏差值，您可以增加或减少该标记被生成的概率。

例如，使用`--logit-bias 15043+1`来增加标记'Hello'出现的可能性，或者使用`--logit-bias 15043-1`来减少其出现的可能性。使用负无穷大的值，`--logit-bias 15043-inf`可以确保`Hello`标记永远不会被生成。

一个更实用的例子可能是通过将`\`标记（29905）设置为负无穷大来防止生成`\code{begin}`和`\code{end}`，即使用`-l 29905-inf`。 (这是由于在LLaMA模型推理中出现的大量LaTeX代码。)

示例用法：`--logit-bias 29905-inf`

### 随机数生成器种子

- `-s SEED, --seed SEED`: 设置随机数生成器（RNG）的种子（默认：-1，-1 = 随机种子）。

RNG种子用于初始化影响文本生成过程的随机数生成器。通过设置一个特定的种子值，您可以在多次运行中获得一致和可重复的结果，前提是输入和设置相同。这有助于测试、调试或比较不同选项对生成文本的影响，以查看它们何时发生分歧。如果种子设置为小于0的值，将使用随机种子，这将导致每次运行的结果不同。

## 性能调优和内存选项

这些选项有助于提高LLaMA模型的性能和内存使用率。通过调整这些设置，您可以微调模型的行为，使其更好地适应您系统的能力，并实现针对特定用例的最佳性能。

### 线程数

-   `-t N, --threads N`: 设置生成时使用的线程数。为了获得最佳性能，建议将此值设置为系统物理CPU核心数（而不是逻辑核心数）。使用正确的线程数可以大大提高性能。
-   `-tb N, --threads-batch N`: 设置批量处理和提示处理时使用的线程数。在某些系统中，批量处理时使用比生成时更多的线程可能是有益的。如果未指定，用于批量处理的线程数将与用于生成的线程数相同。

### Mlock

-   `--mlock`: 将模型锁定在内存中，防止其在内存映射时被交换出去。这可以提高性能，但需要更多的RAM来运行，并且可能会因为模型加载到RAM中而减慢加载时间，这牺牲了内存映射的一些优点。

### 不进行内存映射

-   `--no-mmap`: 不将模型内存映射。默认情况下，模型会被映射到内存中，这样系统可以按需加载模型的必要部分。然而，如果模型的大小超过了您的总RAM量，或者您的系统可用内存较少，使用mmap可能会增加页面错误的几率，从而对性能产生负面影响。禁用mmap会导致加载时间变慢，但可能会减少页面错误，如果您没有使用`--mlock`。请注意，如果模型的大小超过了总RAM量，关闭mmap将完全阻止模型加载。

### NUMA支持

-   `--numa distribute`: 将线程均匀分配到每个NUMA节点上的核心。这将把负载分散到系统上的所有核心，以牺牲可能需要跨节点之间慢速链接的内存传输为代价。
-   `--numa isolate`: 将所有线程都绑定到程序启动的NUMA节点上。这将限制可用的核心数量和内存量，但保证所有内存访问都保持在本地的NUMA节点上。
-   `--numa numactl`: 将线程绑定到由numactl实用程序传递给程序的CPUMAP上。这是最灵活的模式，允许任意核心使用模式，例如使用一个映射，它使用一个NUMA节点的所有核心，并在第二个节点上使用足够的核心来饱和节点间的内存总线。

这些标志尝试进行优化，帮助在某些具有非统一内存访问的系统上。这目前包括上述策略之一，并禁用mmap的预取和读取。后者会导致映射的页面在第一次访问时才发生错误，而不是一次性全部发生，并且与绑定线程到NUMA节点结合使用，更多的页面最终会停留在它们被使用的NUMA节点上。请注意，如果模型已经在系统页面缓存中，例如由于之前的运行没有使用此选项，这将会产生很小的影响，除非您首先清除页面缓存。这可以通过重新启动系统或在Linux上以root权限写入'3'到`/proc/sys/vm/drop_caches`来实现。

### 内存浮点 32

-   `--memory-f32`: 使用32位浮点数代替16位浮点数进行内存键+值。这将使上下文内存需求加倍，缓存的提示文件大小也加倍，但似乎没有以可衡量的方式增加生成质量。不推荐使用。

### 批处理大小

-   `-b N, --batch-size N`: 设置提示处理的批处理大小（默认：`2048`）。这个大的批处理大小对安装了BLAS并启用了BLAS的用户有利。如果您没有启用BLAS（"BLAS=0"），您可以使用较小的数字，例如8，以查看在某些情况下提示的评估进度。

- `-ub N`, `--ubatch-size N`: 物理最大批处理大小。这是为了管道并行化。默认：`512`。

### 提示缓存

-   `--prompt-cache FNAME`: 指定一个文件来缓存初始提示后的模型状态。这可以在使用较长的提示时显著加快启动时间。该文件在第一次运行时创建，并在后续运行中重复使用和更新。**注意**：恢复缓存的提示不意味着恢复保存时的确切会话状态。因此，即使指定了特定的种子，也无法保证获得与原始生成相同的标记序列。

### 语法与JSON模式

-   `--grammar GRAMMAR`, `--grammar-file FILE`: 指定一个语法（定义在行内或文件中）来限制模型输出为特定格式。例如，你可以强制模型输出JSON或只使用表情符号。请参阅[GBNF指南](../../grammars/README.md)以获取语法细节。

-   `--json-schema SCHEMA`: 指定一个[JSON模式](https://json-schema.org/)来限制模型输出。例如，`{}`可用于任何JSON对象，或`{"items": {"type": "string", "minLength": 10, "maxLength": 100}, "minItems": 10}`用于具有大小限制的字符串JSON数组。如果模式使用外部`$ref`，则应使用`--grammar "$( python examples/json_schema_to_grammar.py myschema.json )"`。

### 量化

关于4位量化，它可以显著提高性能并减少内存使用，请参阅llama.cpp的主要[README](../../README.md#prepare-and-quantize)。

## 其他选项

这些选项在运行LLaMA模型时提供额外的功能和定制：

-   `-h, --help`: 显示一个帮助消息，显示所有可用选项及其默认值。这特别有用于检查最新的选项和默认值，因为它们可能会频繁更改，并且本文件中的信息可能会过时。
-   `--verbose-prompt`: 在生成文本之前打印提示。
-   `-ngl N, --n-gpu-layers N`: 当与GPU支持编译时，此选项允许将一些层卸载到GPU进行计算。通常会导致性能提高。
-   `-mg i, --main-gpu i`: 当使用多个GPU时，此选项控制哪个GPU用于小型张量，对于所有GPU进行计算的开销不值得。该GPU将使用稍多的VRAM来存储临时结果的缓存区。默认情况下使用GPU 0。
-   `-ts SPLIT, --tensor-split SPLIT`: 当使用多个GPU时，此选项控制大型张量应如何跨所有GPU分割。`SPLIT`是一个逗号分隔的非负值列表，它将分配给每个GPU的数据比例。例如，“3,2”将分配60%的数据给GPU 0，40%给GPU 1。默认情况下，数据根据VRAM成比例分割，但这可能不是性能最佳选择。
-   `--lora FNAME`: 将LoRA（低秩适配）适配器应用到模型中（意味着禁用--no-mmap）。这允许您将预训练模型适应特定的任务或领域。
-   `--lora-base FNAME`: 供LoRA适配器修改的层的基模型（可选）。此标志与`--lora`标志一起使用，并指定了适应的基模型。

-   `-hfr URL --hf-repo URL`: Hugging Face模型仓库的URL。与`--hf-file`或`-hff`一起使用。模型将从提供的文件下载并存储在`-m`或`--model`指定的文件中。如果没有提供`-m`，则模型将自动存储在`LLAMA_CACHE`环境变量指定的路径或特定于OS的本地缓存中。

