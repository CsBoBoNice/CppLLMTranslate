## llama.cpp/example/主程序

此示例程序使您能够轻松高效地使用各种LLaMA语言模型。它专为[llama.cpp](https://github.com/ggerganov/llama.cpp)项目设计，该项目提供了一个基础的C/C++实现，带有可选的4位量化支持，以实现更快、更低内存的推理，并针对桌面CPU进行了优化。这个程序可用于LLaMA模型的各种推断任务，包括根据用户提供的提示生成文本和使用反向提示的聊天式交互。

## 表格目录

1. [快速开始](#quick-start)
2. [常用选项](#common-options)
3. [输入提示](#input-prompts)
4. [交互](#interaction)
5. [上下文管理](#context-management)
6. [生成标志](#generation-flags)
7. [性能调优与内存选项](#performance- tuning-and-memory-options)
8. [其他选项](#additional- options)

## 快速开始

立即开始时，请运行以下命令，确保使用了正确模型路径：

#### Unix系统（Linux、macOS等）：

```bash
./main -m models/7B/ggml-model.bin --prompt "Once upon a time"
```

#### Windows：

```powershell
main.exe -m models\7B\ggml-model.bin --prompt "Once upon a time"
```
为了获得互动体验，可以尝试此命令：

#### Unix系统（Linux、macOS等）：

```bash
./main -m models/7B/ggml-model.bin -n -1 --color -r "User:" --in-prefix " " -i -p \
'User: Hi
AI: Hello. I am an AI chatbot. Would you like to talk?
User: Sure! 
AI: What would you like to talk about?
User:'
```

#### Windows：

```powershell
main.exe -m models\7B\ggml-model.bin -n -1 --color -r "User:" --in-prefix " " -i -e -p "User: Hi\nAI: Hello. I am an AI chatbot. Would you like to talk?\nUser: Sure!\nAI: What would you like to talk about?\nUser:"
```
以下命令根据起始提示生成无限文本（可按 `Ctrl-C` 中断）：

#### Unix系统（Linux、macOS等）：

```bash
./main -m models/7B/ggml-model.bin --ignore-eos -n -1 --random-prompt
```

#### Windows：

```powershell
main.exe -m models\7B\ggml-model.bin --ignore-eos -n -1 --random-prompt
```

## 常用选项

本节介绍使用LLaMA模型运行`main`程序时最常用的选项：

-   `-m FNAME, --model FNAME`: 指定LLaMA模型文件的路径（例如，`models/7B/ggml-模型.gguf`；如果设置`--model-url`，则由其推断）。
-   `--model-url MODEL_URL  -mu MODEL_URL`: 指定从远程http URL下载文件的路径（例如：https://huggingface.co/ggml-org/models/resolve/main/phi-2/ggml-模型- q4_0.gguf）。
-   `--interactive -i`: 以交互模式运行程序，允许你直接输入并实时接收响应。
-   `--instruct -ins`: 在指令模式下运行程序，这对于与Alpaca模型合作特别有用。
-   `--n-predict N -n N`: 设置生成文本时要预测的令牌数量。调整此值会影响生成文本的长度。
-   `--ctx-size N -c N`: 设置提示上下文的大小。默认值为512，但LLaMA模型是为2048的上下文构建的，这对于较长的输入/推理会提供更好的结果。

## 输入提示

`main`程序提供了多种使用输入提示与LLaMA模型交互的方式：

-   `--prompt PROMPT`: 直接在命令行选项中提供提示。
-   `--file FNAME`: 提供包含单个或多个提示的文件。
-   `--interactive-first`: 首先以交互模式运行程序并立即等待输入。更多信息见下文。
-   `--random-prompt`: 从随机提示开始。

## 交互

`main`程序提供了一种无缝的与LLaMA模型交互方式，用户可以进行实时对话或为特定任务提供指令。可以通过多种选项（如`--interactive`、`--interactive- first`和`--instruct`）触发交互模式。
在交互模式下，用户可以通过在过程中的输入参与到文本生成中。用户在任何时候可以按下 `Ctrl+C` 中断并输入，然后按回车键提交给LLaMA模型。要提交多行但不完成输入，可以在当前行尾添加反斜杠（`\`）后继续输入。

### 交互选项

-   `--interactive -i`: 在交互模式下运行程序，用户可以进行实时对话或向模型提供特定指令。
-   `--interactive-first`: 首先以交互模式运行程序，立即在开始文本生成前等待用户输入。
-   `--instruct -ins`: 在指令模式下运行程序，这种模式特别适合与擅长根据用户指令完成任务的Alpaca模型配合。

-   `--color`: 启用彩色输出，以便在视觉上区分提示、用户输入和生成的文本。

通过理解和使用这些交互选项，你可以创建引人入胜且动态的LLaMA模型体验，定制文本生成过程以满足你的特定需求。

### 反馈提示

反馈提示是通过在遇到特定文本字符串时暂停文本生成，从而与LLaMA模型创造类似聊天体验的强大方法：

-   `--reverse-prompt PROMPT -r PROMPT`: 指定一个或多个反向提示，用于暂停文本生成并切换到交互模式。例如，`-r "User:"` 可以用于在轮到用户说话时立即返回对话，有助于创建更互动和交谈式的体验。但请注意，反向提示不能以空格结束。

### 解决方案

为了解决这个限制，你可以使用`--in-prefix`标志在反向提示后面添加空格或其他字符。

### 输入前缀

`--in-prefix`标志用于在输入前添加前缀，主要用来在反向提示后添加空格。例如，结合`--reverse-prompt`标志使用：

```sh
./main -r "User:" --in-prefix "  "
```

### 输入后缀

`--in-suffix`标志用于在输入后添加后缀。这对于在用户输入后添加“Assistant:”提示很有用。它会添加在用户输入末尾的换行符(`\n`)之后。结合`--reverse-prompt`使用示例：

```sh
./main -r "User:" --in-prefix "  " --in-suffix "Assistant:"
```

### 指令模式

在处理Alpaca模型时，指令模式特别有用，这类模型设计用于根据用户提供的指令完成特定任务：

-   `--instruct -ins`: 启用指令模式，以利用Alpaca模型根据用户提供的指令完成任务的能力。
技术细节：用户的输入在内部会以反向提示（或默认为`### 指令:`）开头，然后是`### 回应:`（除非你仅按回车键而没有输入，否则会继续生成较长的回应）。

通过理解和使用这些交互选项，你可以创建引人入胜且动态的LLaMA模型体验，定制文本生成过程以满足你的特定需求。

## 上下文管理

在文本生成过程中，LLaMA模型的上下文大小有限，意味着它们只能处理一定数量的输入和生成文本中的令牌。当上下文填满时，模型会内部重置，可能丢失对话或指令开始时的部分信息。上下文管理选项有助于在这种情况下保持连贯性和一致性。

### 上下文大小

`--ctx-size`选项允许你设置LLaMA模型在文本生成期间使用的提示上下文大小。较大的上下文可以帮助模型更好地理解并生成对较长输入或对话的响应。

-   `--ctx-size N -c N`: 设置提示上下文大小（默认：512）。LLaMA模型构建时的上下文大小为2048，对于较长的输入/推理，这将提供最佳结果。然而，将上下文大小增加到2048以上可能会导致不可预测的结果。

### 扩展上下文大小

一些经过微调的模型通过调整RoPE的长度扩大了上下文长度。例如，原始预训练模型的上下文长度（最大序列长度）为4096（4k），而微调模型则为32k。这是放大因子8，可以通过设置`--ctx-size`为32768（32k）和`--rope-scale`为8来实现。

-   `--rope-scale N`：N是微调模型使用的线性放大因子。

### 保持提示

`--keep`选项允许用户在模型上下文用尽时保留原始提示，确保与初始指令或对话主题的连接得以保持。
-   `--keep N`: 指定在模型内部上下文重置时保留初始提示的令牌数量。默认值为0（即不保留任何令牌）。使用 `-1` 可保留初始提示的所有令牌。

通过使用`--ctx-size`和`--keep`等上下文管理选项，你可以保持与LLaMA模型更加连贯和一致的互动，确保生成的文本与原始提示或对话内容保持相关性。

## 生成标志

以下选项允许你控制文本生成过程，并根据需求调整生成文本的多样性和质量。通过调整这些选项并尝试不同的值组合，你可以找到最适合你的特定应用场景的最佳设置。

### 预测的令牌数

-   `--n-predict N -n N`: 设置生成文本时要预测的令牌数量（默认：128，-1 = 无穷，-2 = 直至上下文填满）

`--n-predict`选项控制模型对输入提示生成文本的响应长度。通过调整此值，你可以影响生成文本的长度。更高的值将产生更长的文本，而较低的值则会产生更短的文本。

值为-1时，将开启无限文本生成，即使我们有有限的上下文窗口。当上下文窗口满时，会丢弃部分早期的令牌（在`--n-keep`之后的一半）。在继续生成之前，上下文必须重新评估。对于大型模型和/或大的上下文窗口，这会导致输出显著暂停。

如果暂停不受欢迎，可以设置`--n-predict`为-2，当上下文填满时立即停止生成。需要注意的是，如果遇到结束序列（EOS）标记或反向提示，生成的文本可能会比指定的令牌数更短。在交互模式下，文本生成会暂停，控制权会交还给用户。非交互模式下，程序会结束。这两种情况下，生成可能在达到指定的`--n-predict`值之前停止。如果你想让模型在不自行生成EOS的情况下继续，可以使用`--ignore-eos`参数。

### 温度

-   `--temp N`: 调整生成文本的随机性（默认：0.8）。

温度是一个超参数，用于控制生成文本的随机性。它影响模型输出令牌的概率分布。较高的温度（例如，1.5）会使输出更随机和富有创造性，而较低的温度（例如，0.5）则使输出更集中、确定且保守。默认值为0.8，提供随机性和确定性的平衡。在极端情况下，温度为0将始终选择最可能的下一个令牌，导致每次运行的输出都相同。

示例用法：`--temp 0.5`

### 重复惩罚

-   `--repeat-penalty N`：控制生成文本中令牌序列重复的程度（默认：1.1）。
-   `--repeat-last-n N`：考虑重复惩罚的最后n个令牌（默认：64，0 = 禁用，-1 = 上下文大小）。
-   `--no-penalize-nl`：在应用重复惩罚时，禁用对换行符的惩罚。

`repeat-penalty`选项有助于防止模型生成重复或单调的文本。较高的值（例如，1.5）将对重复施加更强的惩罚，而较低的值（例如，0.9）将更加宽容。默认值为1.1。
`repeat-last-n`选项控制用于重复惩罚的历史令牌数量。较大的值会使模型在生成的文本中查看更远的地方以防止重复，而较小的值只考虑最近的令牌。值为0禁用惩罚，值为-1则设置考虑的令牌数量等于上下文大小（`ctx-size`）。

使用`--no-penalize-nl`选项在应用重复惩罚时禁用换行符惩罚。此选项在生成聊天对话、对话、代码、诗歌或任何换行符在结构和格式中发挥重要作用的文本时特别有用。禁用换行符惩罚有助于保持这些特定用例中的自然流畅性和预期格式。

示例用法：`--repeat-penalty 1.15 --repeat-last-n 128 --no-penalize-nl`

### 顶部K采样

-   `--top- k N`：将下一个令牌的选择限制在模型预测的最可能的K个令牌上（默认：40）。

顶部K采样是一种文本生成方法，它仅从模型预测的最可能的K个令牌中选择下一个令牌。它有助于减少生成低概率或无意义令牌的风险，但也可能限制输出的多样性。对于顶部K值较高（例如，100），将考虑更多的令牌，从而生成更加多样化的文本；而较低的值（例如，10）则会集中于最可能的令牌，生成更保守的文本。默认值为40。
示例用法：`--top- k 30`

### 顶部P采样

-   `--top- p N`：将下一个令牌的选择限制在概率阈值P以上的令牌子集上（默认：0.9）。

顶部P采样，也称为核采样，是一种文本生成方法，它从总概率至少为p的令牌子集中选择下一个令牌。这种方法在多样性与质量之间找到了平衡，同时考虑了令牌的概率和采样的令牌数量。较高的top-p值（例如，0.95）将导致更多样化的文本，而较低的值（例如，0.5）则会生成更集中和保守的文本。默认值为0.9。

示例用法：`--min- p 0.05`

### 最小P采样

-   `--min- p N`：设置令牌选择的最小基本概率阈值（默认：0.05）。

最小P采样方法是Top-P的替代方案，旨在保证质量和多样性的平衡。参数*p*表示一个令牌被考虑的最小概率，相对于最可能令牌的概率。例如，设*p*=0.05，而最可能的令牌的概率为0.9，则概率低于0.045的令牌会被过滤掉。
示例用法：`--tfs 1.0`（1.0 = 禁用）

### 尾部自由采样（TFS）

-   `--tfs N`：启用尾部自由采样，参数z（默认：1.0，1.0 = 禁用）。

尾部自由采样（TFS）是一种文本生成技术，旨在减少不太可能的令牌（可能不相关、不连贯或无意义）对输出的影响。与Top-P类似，它试图动态地确定最可能的令牌的大部分。然而，TFS基于概率的二阶导数过滤掉令牌。添加令牌后，当二阶导数之和达到参数z时停止。简而言之：TFS通过参数z观察令牌概率下降的速度，然后使用尾部去除不太可能的令牌。z的典型值范围在0.9到0.95之间。值1.0将包括所有令牌，从而禁用TFS的效果。

示例用法：`--typical 1.0`（1.0 = 禁用）

### 当地典型采样

-   `--typical N`：启用基于上下文的典型采样，参数p（默认：1.0，1.0 = 禁用）。

当地典型采样通过基于周围上下文选取典型或预期的令牌，以促进生成语境连贯且多样的文本。通过设置p在0到1之间，你可以控制生成文本的本地连贯性和多样性之间的平衡。接近1的值会促进更语境连贯的令牌，而接近0的值则会促进更多样化的令牌。设置为1将禁用当地典型采样。

示例用法：`--mirostat 1`（1 = Mirostat，2 = Mirostat 2.0）

### Mirostat采样

-   `--mirostat N`：启用Mirostat采样，控制生成文本时的困惑度（默认：0，0 = 禁用，1 = Mirostat，2 = Mirostat 2.0）。
-   `--mirostat- lr N`：设置Mirostat的学习率，参数η（默认：0.1）。
-   `--mirostat-ent N`：设置Mirostat的目标熵，参数τ（默认：5.0）。

Mirostat是一种算法，它在文本生成过程中动态保持生成文本的质量在预设范围内。它旨在在连贯性和多样性之间取得平衡，避免由过度重复（厌倦陷阱）或不连贯（困惑陷阱）导致的低质量输出。

`--mirostat- lr`选项设置Mirostat的学习率（η）。学习率影响算法对生成文本反馈的响应速度。较低的学习率会导致调整速度较慢，而较高的学习率会使算法反应更加灵敏。默认值为`0.1`。

`--mirostat-ent`选项设置Mirostat的目标熵（τ），它代表生成文本期望的困惑度值。调整目标熵允许你控制生成文本的连贯性和多样性之间的平衡。较低的值将产生更专注且连贯的文本，而较高的值将导致更多样，但可能不太连贯的文本。默认值为`5.0`。

示例用法：`--mirostat 2 --mirostat- lr 0.05 --mirostat-ent 3.0`（注：这些选项的翻译保持原样，因为它们是命令行参数）

### 逻辑偏置

-   `-l TOKEN_ ID(+/-)BIAS, --logit- bias TOKEN_ ID(+/-)BIAS`：修改生成文本完成时特定令牌出现的概率。

逻辑偏置选项允许你手动调整生成文本中特定令牌出现的概率。通过提供一个令牌ID和正或负的偏置值，你可以增加或减少该令牌被生成的概率。

例如，使用`--logit- bias 15043+1`增加令牌'Hello'出现的可能性，或使用`--logit- bias 15043-1`降低其可能性。使用负无穷值`--logit- bias 15043-inf`会确保永不产生令牌'Hello'。


一个实际的应用场景可能是，通过将`29905`令牌（用于`\`）设置为负无穷大，防止生成`\code{begin}`和`\code{end}`。这是因为LLaMA模型推理中经常出现LaTeX代码。使用方法：`--logit- bias 29905-inf`。

### 随机数生成器种子

-   `-s SEED, --seed SEED`：设置随机数生成器（RNG）种子（默认：-1，-1 = 随机种子）。

随机数生成器种子用于初始化影响文本生成过程的随机数生成器。通过设置特定的种子值，你可以确保在多次运行中，使用相同的输入和设置下，结果是可重复且一致的。这对于测试、调试或比较不同选项对生成文本的影响时，找出它们何时产生差异非常有帮助。如果种子设置为小于0的值，将使用随机种子，每次运行的输出将不同。

## 性能调优和内存选项

这些选项有助于优化LLaMA模型的性能和内存使用。通过调整这些设置，你可以针对系统的功能进行模型行为的微调，从而为特定使用场景实现最佳性能。

### 线程数

-   `-t N, --threads N`：设置生成时使用的线程数。为了达到最佳性能，推荐设置为系统拥有的物理CPU核心数（而不是逻辑核心数）。正确选择线程数可以极大地提升性能。

-   `-tb N, --threads- batch N`：设置用于批处理和提示处理的线程数。在某些系统中，批处理时使用更多线程可能有益。如果不指定，批处理和生成使用的线程数将保持一致。

### Mlock锁定

-   `--mlock`：将模型锁定在内存中，防止在内存映射时被交换出去。这可以提高性能，但以牺牲内存映射的部分优势为代价，因为它需要更多的RAM来运行，并可能由于模型加载到内存而延长加载时间。

### 不使用内存映射

-   `--no- mmap`：不要将模型内存映射。默认情况下，模型会被映射到内存中，系统可以根据需要加载模型的必要部分。然而，如果模型大于你的总RAM，或者你的系统内存紧张，使用内存映射可能会增加页面出错（pageouts）的风险，从而影响性能。禁用内存映射会导致加载速度变慢，但如果未使用`--mlock`，可能会减少页面出错。需要注意的是，如果模型大于总RAM，禁用内存映射将阻止模型的加载。

### NUMA支持

-   `--numa distribute`：将一个等比例的线程分配到每个NUMA节点的核心上。这将负载均匀地分配到系统的所有核心，充分利用所有内存通道，但可能需要内存在节点之间通过慢速链接传输。
-   `--numa isolate`：将所有线程固定在程序启动的NUMA节点上。这会限制可用的核心数和内存量，但确保所有内存访问都保持在NUMA节点的本地范围内。

-   `--numa numactl`：使用numactl工具启动程序时，将线程绑定到传递给程序的CPUMAP。这是最灵活的模式，允许使用任意核心模式，例如在一个NUMA节点上使用所有核心，而在第二个节点上仅使用足够的核心以饱和跨节点内存总线。

这些标志尝试进行优化，有助于在具有非均匀内存访问的某些系统上运行。目前，这包括上述策略之一，并禁用内存映射的预读取和预读。这样，映射的页面会在首次访问时而不是一次性加载，与线程绑定到NUMA节点结合，将更多的页面放在它们被使用的NUMA节点上。请注意，如果模型已经在系统的缓存页中，例如由于之前没有使用此选项运行，除非先清除缓存，否则这将影响不大。在Linux上，可以通过以root权限写入`'/proc/sys/vm/drop_caches'`为'3'来实现。

### 内存使用32位浮点数

-   `--memory- f32`：使用32位浮点数而不是16位浮点数来存储内存键值对。这将增加上下文内存需求和缓存的提示文件大小一倍，但似乎并没有在可测量的方式下提高生成质量。不推荐使用。

### 批次大小

-   `-b N, --batch-size N`：设置提示处理的批次大小（默认：`2048`）。对于安装并启用了BLAS（构建时）的用户，较大的批次大小是有益的。如果没有启用BLAS（"BLAS=0"），你可以使用较小的数字，如8，以便在某些情况下观察提示的处理进度。
-   `-ub N`, `--ubatch- size N`：物理最大批次大小。用于管道并行化。默认值：`512`。

### 提示缓存

-   `--prompt- cache FNAME`：指定一个文件，用于在初始提示后缓存模型状态。这对于使用较长提示时可以显著加快启动时间。文件在首次运行时创建，并在后续运行中重用和更新。**注意**：使用缓存的提示并不意味着恢复到保存时的会话确切状态。即使指定特定的种子，你也无法保证得到与原始生成完全相同的令牌序列。

### 语法及JSON模式

-   `--grammar GRAMMAR` 或 `--grammar- file FILE`：指定一个语法（定义在内联或文件中），用于限制模型的输出格式。例如，可以让你的模型只输出JSON或只使用表情符号。请参阅[GBNF指南](../../grammars/README. md)了解详细语法。

-   `--json- schema SCHEMA`：指定一个[JSON模式](https://json-schema.org/)，来限定模型输出的格式（例如：`{}`表示任何JSON对象，或`{"items": {"type": "string", "minLength": 10, "maxLength": 100}, "minItems": 10}`表示有长度限制的字符串数组）。如果模式使用了外部`$ref`，你应该使用`--grammar "$( python examples/json_schema_ to_grammar.py myschema.json )" `代替。

### 量化

关于可以显著提高性能并减少内存使用的4位量化信息，请参阅`llama.cpp`的主[README](../../README. md#prepare- and-quantize)。

## 其他选项

这些选项提供了在运行LLaMA模型时的额外功能和自定义选项：

-   `-h, --help`：显示包含所有可用选项及其默认值的帮助信息。这对于检查最新的选项和默认值特别有用，因为它们可能会经常变化，而本文档中的信息可能会过时。
-   `--verbose- prompt`：在生成文本前打印提示。
-   `-ngl N, --n-gpu-layers N`：当编译支持GPU时，此选项允许将一些层卸载到GPU进行计算。通常会提高性能。
-   `--main-gpu i` 或 `-mg i`：在使用多GPU时，此选项控制用于那些在所有GPU间分散计算开销不值得的小张量的GPU。相应的GPU会使用更多VRAM来存储临时结果的缓存缓冲区。默认使用GPU 0。
-   `-ts SPLIT, --tensor- split SPLIT`：当使用多GPU时，此选项控制大张量应如何在所有GPU间划分。`SPLIT`是一个逗号分隔的非负值列表，用于指定每个GPU应获得的数据比例。例如，"3,2"将分配60%的数据给GPU 0，40%给GPU 1。默认情况下，数据按VRAM比例划分，但这可能不是性能最优的方式。

-   `--lora FNAME`：对模型应用LoRA（低秩适应）适配器（意味着`--no-mmap`）。这允许你将预训练模型适配到特定任务或领域。
-   `--lora-base FNAME`：可选基础模型，用于LoRA适配器修改的层。此标志与`--lora`标志一起使用，指定适应的基础模型。
